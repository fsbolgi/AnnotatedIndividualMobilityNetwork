{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANTIC CLUSTERING OF THE LOCATIONS\n",
    "# (USING THE LOG DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import KMeans\n",
    "from pandas.plotting import parallel_coordinates\n",
    "import pickle\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "import selenium.webdriver\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "\n",
    "plt.rcParams[\"font.family\"] = 'serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the parameters to select the correct area and time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = '5'\n",
    "id_area = '11'\n",
    "month =  '9'\n",
    "n_months = '2'\n",
    "week = '0'\n",
    "\n",
    "month_code = month\n",
    "if n_months != \"1\":\n",
    "    for m in range(1, int(n_months)):\n",
    "        month_code += \"_\" + str(int(month)+m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the dataframe of the location features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of different vehicles is 279\n",
      "the total number of locations is 38770\n"
     ]
    }
   ],
   "source": [
    "path = '../../../datasets/out/Traj' + stop + 'min/'\n",
    "file_name_in = 'loc_feat_area'+id_area+'_month'+month_code+'_week'+ week + '_compl_log_norm.csv'\n",
    "file_name_out = '_area'+id_area+'_month'+month_code+'_week'+ week + '_log'\n",
    "\n",
    "df = pd.read_csv(path+file_name_in)\n",
    "\n",
    "print(\"the number of different vehicles is\", len(df[\"vehicle\"].unique()))\n",
    "print(\"the total number of locations is\", len(df[\"vehicle\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>is_regular</th>\n",
       "      <th>loc_proto_lat</th>\n",
       "      <th>loc_proto_lon</th>\n",
       "      <th>n_next_locs</th>\n",
       "      <th>radius</th>\n",
       "      <th>entropy</th>\n",
       "      <th>support</th>\n",
       "      <th>avg_stay_weekday_day</th>\n",
       "      <th>avg_stay_weekend_day</th>\n",
       "      <th>...</th>\n",
       "      <th>k_supermarket</th>\n",
       "      <th>d_gas</th>\n",
       "      <th>d_parking</th>\n",
       "      <th>d_pier</th>\n",
       "      <th>d_hotel</th>\n",
       "      <th>d_food</th>\n",
       "      <th>d_leisure</th>\n",
       "      <th>d_shop</th>\n",
       "      <th>d_service</th>\n",
       "      <th>d_supermarket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>114.151896</td>\n",
       "      <td>0.152437</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.612036</td>\n",
       "      <td>0.204463</td>\n",
       "      <td>0.187574</td>\n",
       "      <td>0.247297</td>\n",
       "      <td>0.138498</td>\n",
       "      <td>0.355065</td>\n",
       "      <td>0.039307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332319</td>\n",
       "      <td>0.711755</td>\n",
       "      <td>0.823826</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.922046</td>\n",
       "      <td>0.571144</td>\n",
       "      <td>0.550508</td>\n",
       "      <td>0.585230</td>\n",
       "      <td>0.532083</td>\n",
       "      <td>0.693427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>99.768370</td>\n",
       "      <td>0.359449</td>\n",
       "      <td>0.106007</td>\n",
       "      <td>0.097147</td>\n",
       "      <td>0.123441</td>\n",
       "      <td>0.175353</td>\n",
       "      <td>0.419232</td>\n",
       "      <td>0.138034</td>\n",
       "      <td>0.220812</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238979</td>\n",
       "      <td>0.217526</td>\n",
       "      <td>0.214722</td>\n",
       "      <td>0.064156</td>\n",
       "      <td>0.171837</td>\n",
       "      <td>0.156337</td>\n",
       "      <td>0.150086</td>\n",
       "      <td>0.177131</td>\n",
       "      <td>0.125784</td>\n",
       "      <td>0.215930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379897</td>\n",
       "      <td>0.557139</td>\n",
       "      <td>0.148634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066345</td>\n",
       "      <td>0.214869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.554301</td>\n",
       "      <td>0.624513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473731</td>\n",
       "      <td>0.461404</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.455646</td>\n",
       "      <td>0.540578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442026</td>\n",
       "      <td>0.614171</td>\n",
       "      <td>0.148634</td>\n",
       "      <td>0.188196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066345</td>\n",
       "      <td>0.422964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.640304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577914</td>\n",
       "      <td>0.531730</td>\n",
       "      <td>0.579384</td>\n",
       "      <td>0.537708</td>\n",
       "      <td>0.635569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>166.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515727</td>\n",
       "      <td>0.661721</td>\n",
       "      <td>0.235580</td>\n",
       "      <td>0.306406</td>\n",
       "      <td>0.722635</td>\n",
       "      <td>0.204986</td>\n",
       "      <td>0.488532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.657168</td>\n",
       "      <td>0.605680</td>\n",
       "      <td>0.657721</td>\n",
       "      <td>0.608016</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             loc_id    is_regular  loc_proto_lat  loc_proto_lon   n_next_locs  \\\n",
       "count  38770.000000  38770.000000   38770.000000   38770.000000  38770.000000   \n",
       "mean     114.151896      0.152437       0.443503       0.612036      0.204463   \n",
       "std       99.768370      0.359449       0.106007       0.097147      0.123441   \n",
       "min        0.000000      0.000000       0.000000       0.000000      0.000000   \n",
       "25%       37.000000      0.000000       0.379897       0.557139      0.148634   \n",
       "50%       87.000000      0.000000       0.442026       0.614171      0.148634   \n",
       "75%      166.000000      0.000000       0.515727       0.661721      0.235580   \n",
       "max      594.000000      1.000000       1.000000       1.000000      1.000000   \n",
       "\n",
       "             radius       entropy       support  avg_stay_weekday_day  \\\n",
       "count  38770.000000  38770.000000  38770.000000          38770.000000   \n",
       "mean       0.187574      0.247297      0.138498              0.355065   \n",
       "std        0.175353      0.419232      0.138034              0.220812   \n",
       "min        0.000000      0.000000      0.000000              0.000000   \n",
       "25%        0.000000      0.000000      0.066345              0.214869   \n",
       "50%        0.188196      0.000000      0.066345              0.422964   \n",
       "75%        0.306406      0.722635      0.204986              0.488532   \n",
       "max        1.000000      1.000000      1.000000              1.000000   \n",
       "\n",
       "       avg_stay_weekend_day  ...  k_supermarket         d_gas     d_parking  \\\n",
       "count          38770.000000  ...   38770.000000  38770.000000  38770.000000   \n",
       "mean               0.039307  ...       0.332319      0.711755      0.823826   \n",
       "std                0.148760  ...       0.238979      0.217526      0.214722   \n",
       "min                0.000000  ...       0.000000      0.000000      0.000000   \n",
       "25%                0.000000  ...       0.000000      0.554301      0.624513   \n",
       "50%                0.000000  ...       0.333333      0.640304      1.000000   \n",
       "75%                0.000000  ...       0.528321      1.000000      1.000000   \n",
       "max                1.000000  ...       1.000000      1.000000      1.000000   \n",
       "\n",
       "             d_pier       d_hotel        d_food     d_leisure        d_shop  \\\n",
       "count  38770.000000  38770.000000  38770.000000  38770.000000  38770.000000   \n",
       "mean       0.992157      0.922046      0.571144      0.550508      0.585230   \n",
       "std        0.064156      0.171837      0.156337      0.150086      0.177131   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        1.000000      1.000000      0.473731      0.461404      0.481486   \n",
       "50%        1.000000      1.000000      0.577914      0.531730      0.579384   \n",
       "75%        1.000000      1.000000      0.657168      0.605680      0.657721   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          d_service  d_supermarket  \n",
       "count  38770.000000   38770.000000  \n",
       "mean       0.532083       0.693427  \n",
       "std        0.125784       0.215930  \n",
       "min        0.000000       0.000000  \n",
       "25%        0.455646       0.540578  \n",
       "50%        0.537708       0.635569  \n",
       "75%        0.608016       1.000000  \n",
       "max        1.000000       1.000000  \n",
       "\n",
       "[8 rows x 73 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the columns for the vehicle and the location id that are not relevant right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.copy()\n",
    "df_corr.drop(['vehicle', 'loc_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We plot the distribution of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw the distribution of the attributes \n",
    "fig = plt.figure(figsize=(100, 100)) \n",
    "fig_dims = (8, 9)\n",
    "\n",
    "plot_type = [\"line\" for i in df_corr.keys()]\n",
    "plot_type[0:2] = [\"pie\", \"ignore\", \"ignore\"]\n",
    "\n",
    "#plot_type = [\"pie\", \"ignore\", \"ignore\", \"bar\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \n",
    "          #  \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \n",
    "           # \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"line\", \"line\", \"line\", \n",
    "          #   \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\"]\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "\n",
    "skip = 0\n",
    "\n",
    "for i in range(len(df_corr.keys())-1):\n",
    "    k = df_corr.keys()[i]\n",
    "    t = plot_type[i]\n",
    "    ax = plt.subplot2grid(fig_dims, (int((i-skip)/9), (i-skip)%9))\n",
    "    \n",
    "    if t == \"pie\":\n",
    "        labels = 'Not Regular', 'Regular'\n",
    "        sizes = df_corr[k].value_counts()\n",
    "        explode = (0, 0.05)\n",
    "        c = [\"#97c170\", \"#dde37a\", \"#e1bd66\", \"#EAC435\"]\n",
    "        inside, texts, ltexts = ax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=50, colors=c)\n",
    "        for i in range(len(texts)):\n",
    "            texts[i].set_fontsize(12)\n",
    "            ltexts[i].set_fontsize(12)\n",
    "        ax.axis('equal') \n",
    "        plt.title(k)\n",
    "        \n",
    "    if t == \"line\":\n",
    "        x = range(0, len(df_corr))\n",
    "        y = sorted(df_corr[k])\n",
    "        plt.plot(x, y, color = '#EAC435', linewidth=2.5)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.xlabel(\"locations\", fontsize=13)\n",
    "        plt.ylabel(k, fontsize=13)\n",
    "        plt.grid(True)\n",
    "        plt.title(k)\n",
    "    \n",
    "    if t == \"bar\":\n",
    "        x = range(0, len(df_corr))\n",
    "        y = sorted(df_corr[k])\n",
    "        _, bins, _ = plt.hist(df_corr[k], 20, color = '#97c170', ec='#FFFFFF')\n",
    "        ax.set_xlabel(k, fontsize=13)\n",
    "        ax.set_ylabel(\"number of locations\", fontsize=13)\n",
    "        plt.title(k)\n",
    "\n",
    "    if t == \"ignore\":\n",
    "        skip += 1\n",
    "        \n",
    "plt.savefig('../../../thesis/images/distribution'+file_name_out+'_minmax.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the correlation plot of the individual, collective and geographical features to understand if some attributes are redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color map from purple to orange\n",
    "cmap = cm.get_cmap('PuOr')\n",
    "\n",
    "plt.rcParams[\"font.size\"] = '16'\n",
    "\n",
    "# draw the heatmap first for the individual features, and then for the collective and the geographical\n",
    "# the correlation between the individual and the others are almost none\n",
    "# and this way we work with 2 smaller matrices\n",
    "\n",
    "# individual heatmap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(40,30)\n",
    "\n",
    "# take only the first part of the dataset\n",
    "correlati = df_corr.iloc[ : , :35].corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "# little trick to solve the bug that the heatmap is cut \n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_indiv.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# collective and geographical heatmap\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(40,30)\n",
    "\n",
    "# take only the second and third part of the dataset\n",
    "correlati = df_corr.iloc[ : , 35:].corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "# little trick to solve the bug that the heatmap is cut \n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_coll_geo.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can remove the attributes that have a high correlation with another\n",
    "\n",
    "In some cases we perform a mean of the correlated columns, in other cases, if the information is just redundant, we just remove the attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.drop([\"support\"], axis=1, inplace=True)\n",
    "\n",
    "avg_stay_weekday = (df_corr[\"avg_stay_weekday_day\"] + df_corr[\"avg_stay_weekday_night\"])/2\n",
    "avg_stay_weekend = (df_corr[\"avg_stay_weekend_day\"] + df_corr[\"avg_stay_weekend_night\"])/2\n",
    "std_stay_weekday = (df_corr[\"std_stay_weekday_day\"] + df_corr[\"std_stay_weekday_night\"])/2\n",
    "std_stay_weekend = (df_corr[\"std_stay_weekend_day\"] + df_corr[\"std_stay_weekend_night\"])/2\n",
    "\n",
    "df_corr = df_corr.assign(avg_stay_weekday=avg_stay_weekday, avg_stay_weekend=avg_stay_weekend,\n",
    "                         std_stay_weekday=std_stay_weekday, std_stay_weekend=std_stay_weekend)\n",
    "\n",
    "df_corr.drop([\"avg_stay_weekday_day\", \"avg_stay_weekday_night\", \"avg_stay_weekend_day\", \"avg_stay_weekend_night\",\n",
    "              \"std_stay_weekday_day\", \"std_stay_weekday_night\", \"std_stay_weekend_day\", \"std_stay_weekend_night\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "avg_time_weekday_day = (df_corr[\"avg_leave_weekday_day\"] + df_corr[\"avg_arrive_weekday_day\"])/2\n",
    "avg_time_weekend_day = (df_corr[\"avg_leave_weekend_day\"] + df_corr[\"avg_arrive_weekend_day\"])/2\n",
    "avg_time_weekday_night = (df_corr[\"avg_leave_weekday_night\"] + df_corr[\"avg_arrive_weekday_night\"])/2\n",
    "avg_time_weekend_night = (df_corr[\"avg_leave_weekend_night\"] + df_corr[\"avg_arrive_weekend_night\"])/2\n",
    "\n",
    "df_corr = df_corr.assign(avg_time_weekday_day=avg_time_weekday_day, avg_time_weekend_day=avg_time_weekend_day,\n",
    "                         avg_time_weekday_night=avg_time_weekday_night, avg_time_weekend_night=avg_time_weekend_night)\n",
    "\n",
    "df_corr.drop([\"avg_leave_weekday_day\", \"avg_arrive_weekday_day\", \"avg_leave_weekend_day\", \"avg_arrive_weekend_day\",\n",
    "              \"avg_leave_weekday_night\", \"avg_arrive_weekday_night\", \"avg_leave_weekend_night\", \"avg_arrive_weekend_night\"], axis=1, inplace=True)\n",
    "\n",
    "df_corr.drop([\"avg_leave_mov_duration\", \"avg_arrive_mov_duration\", \"std_leave_mov_duration\", \"std_arrive_mov_duration\"], axis=1, inplace=True)\n",
    "\n",
    "df_corr.drop([\"centrality5K\", \"rev_centrality3\", \"rev_centrality8\", \"rev_centrality10\"], axis=1, inplace=True)\n",
    "\n",
    "# move the collective features as the last columns of the dataframe\n",
    "columns_df_c = [\"exclusivity\", \"centrality1K\", \"centrality15K\", \"rev_centrality1\", \"rev_centrality5\", \"rev_centrality20\"]\n",
    "df_corr = df_corr[[c for c in df_corr if c not in columns_df_c] + [c for c in columns_df_c if c in df_corr]]\n",
    "\n",
    "# move the geographical features as the last columns of the dataframe\n",
    "categories = [\"gas\", \"parking\", \"pier\", \"hotel\", \"food\", \"leisure\", \"shop\", \"service\", \"supermarket\"]\n",
    "columns_df_g = [\"n_\"+c for c in categories]+[\"k_\"+c for c in categories]+[\"d_\"+c for c in categories]\n",
    "df_corr = df_corr[[c for c in df_corr if c not in columns_df_g] + [c for c in columns_df_g if c in df_corr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_regular</th>\n",
       "      <th>loc_proto_lat</th>\n",
       "      <th>loc_proto_lon</th>\n",
       "      <th>n_next_locs</th>\n",
       "      <th>radius</th>\n",
       "      <th>entropy</th>\n",
       "      <th>n_stop_weekday_day</th>\n",
       "      <th>n_stop_weekend_day</th>\n",
       "      <th>n_stop_weekday_night</th>\n",
       "      <th>n_stop_weekend_night</th>\n",
       "      <th>...</th>\n",
       "      <th>k_supermarket</th>\n",
       "      <th>d_gas</th>\n",
       "      <th>d_parking</th>\n",
       "      <th>d_pier</th>\n",
       "      <th>d_hotel</th>\n",
       "      <th>d_food</th>\n",
       "      <th>d_leisure</th>\n",
       "      <th>d_shop</th>\n",
       "      <th>d_service</th>\n",
       "      <th>d_supermarket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "      <td>38770.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.152437</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.612036</td>\n",
       "      <td>0.204463</td>\n",
       "      <td>0.187574</td>\n",
       "      <td>0.247297</td>\n",
       "      <td>0.215138</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>0.031905</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332319</td>\n",
       "      <td>0.711755</td>\n",
       "      <td>0.823826</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.922046</td>\n",
       "      <td>0.571144</td>\n",
       "      <td>0.550508</td>\n",
       "      <td>0.585230</td>\n",
       "      <td>0.532083</td>\n",
       "      <td>0.693427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.359449</td>\n",
       "      <td>0.106007</td>\n",
       "      <td>0.097147</td>\n",
       "      <td>0.123441</td>\n",
       "      <td>0.175353</td>\n",
       "      <td>0.419232</td>\n",
       "      <td>0.134127</td>\n",
       "      <td>0.088196</td>\n",
       "      <td>0.104546</td>\n",
       "      <td>0.057194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238979</td>\n",
       "      <td>0.217526</td>\n",
       "      <td>0.214722</td>\n",
       "      <td>0.064156</td>\n",
       "      <td>0.171837</td>\n",
       "      <td>0.156337</td>\n",
       "      <td>0.150086</td>\n",
       "      <td>0.177131</td>\n",
       "      <td>0.125784</td>\n",
       "      <td>0.215930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379897</td>\n",
       "      <td>0.557139</td>\n",
       "      <td>0.148634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.554301</td>\n",
       "      <td>0.624513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473731</td>\n",
       "      <td>0.461404</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.455646</td>\n",
       "      <td>0.540578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442026</td>\n",
       "      <td>0.614171</td>\n",
       "      <td>0.148634</td>\n",
       "      <td>0.188196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.640304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577914</td>\n",
       "      <td>0.531730</td>\n",
       "      <td>0.579384</td>\n",
       "      <td>0.537708</td>\n",
       "      <td>0.635569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515727</td>\n",
       "      <td>0.661721</td>\n",
       "      <td>0.235580</td>\n",
       "      <td>0.306406</td>\n",
       "      <td>0.722635</td>\n",
       "      <td>0.250312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.657168</td>\n",
       "      <td>0.605680</td>\n",
       "      <td>0.657721</td>\n",
       "      <td>0.608016</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         is_regular  loc_proto_lat  loc_proto_lon   n_next_locs        radius  \\\n",
       "count  38770.000000   38770.000000   38770.000000  38770.000000  38770.000000   \n",
       "mean       0.152437       0.443503       0.612036      0.204463      0.187574   \n",
       "std        0.359449       0.106007       0.097147      0.123441      0.175353   \n",
       "min        0.000000       0.000000       0.000000      0.000000      0.000000   \n",
       "25%        0.000000       0.379897       0.557139      0.148634      0.000000   \n",
       "50%        0.000000       0.442026       0.614171      0.148634      0.188196   \n",
       "75%        0.000000       0.515727       0.661721      0.235580      0.306406   \n",
       "max        1.000000       1.000000       1.000000      1.000000      1.000000   \n",
       "\n",
       "            entropy  n_stop_weekday_day  n_stop_weekend_day  \\\n",
       "count  38770.000000        38770.000000        38770.000000   \n",
       "mean       0.247297            0.215138            0.025094   \n",
       "std        0.419232            0.134127            0.088196   \n",
       "min        0.000000            0.000000            0.000000   \n",
       "25%        0.000000            0.170865            0.000000   \n",
       "50%        0.000000            0.170865            0.000000   \n",
       "75%        0.722635            0.250312            0.000000   \n",
       "max        1.000000            1.000000            1.000000   \n",
       "\n",
       "       n_stop_weekday_night  n_stop_weekend_night  ...  k_supermarket  \\\n",
       "count          38770.000000          38770.000000  ...   38770.000000   \n",
       "mean               0.031905              0.009491  ...       0.332319   \n",
       "std                0.104546              0.057194  ...       0.238979   \n",
       "min                0.000000              0.000000  ...       0.000000   \n",
       "25%                0.000000              0.000000  ...       0.000000   \n",
       "50%                0.000000              0.000000  ...       0.333333   \n",
       "75%                0.000000              0.000000  ...       0.528321   \n",
       "max                1.000000              1.000000  ...       1.000000   \n",
       "\n",
       "              d_gas     d_parking        d_pier       d_hotel        d_food  \\\n",
       "count  38770.000000  38770.000000  38770.000000  38770.000000  38770.000000   \n",
       "mean       0.711755      0.823826      0.992157      0.922046      0.571144   \n",
       "std        0.217526      0.214722      0.064156      0.171837      0.156337   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.554301      0.624513      1.000000      1.000000      0.473731   \n",
       "50%        0.640304      1.000000      1.000000      1.000000      0.577914   \n",
       "75%        1.000000      1.000000      1.000000      1.000000      0.657168   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          d_leisure        d_shop     d_service  d_supermarket  \n",
       "count  38770.000000  38770.000000  38770.000000   38770.000000  \n",
       "mean       0.550508      0.585230      0.532083       0.693427  \n",
       "std        0.150086      0.177131      0.125784       0.215930  \n",
       "min        0.000000      0.000000      0.000000       0.000000  \n",
       "25%        0.461404      0.481486      0.455646       0.540578  \n",
       "50%        0.531730      0.579384      0.537708       0.635569  \n",
       "75%        0.605680      0.657721      0.608016       1.000000  \n",
       "max        1.000000      1.000000      1.000000       1.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can compute the correlation matrix again after the varible transformation (in this case we draw only one heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix only with the collective and geographic features\n",
    "\n",
    "cmap = cm.get_cmap('PuOr')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(60, 50)\n",
    "\n",
    "correlati = df_corr.corr()\n",
    "correlati = correlati.round(2)\n",
    "ax = sns.heatmap(correlati, cmap=cmap, vmin = -1, vmax = 1, annot = True,linewidths=.4)\n",
    "\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.savefig('../../../thesis/images/corr'+file_name_out+'_after.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS CLUSTERING OF THE FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the sse and the silhouette for k in the range from 2 to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 2-10, step 2\n",
      "range 10-50, step 5\n",
      "range 50-200, step 15\n",
      "range 200-1000, step 100\n"
     ]
    }
   ],
   "source": [
    "############################ DO NOT RUN AGAIN, TOO LONG ############################\n",
    "sse_list = list()\n",
    "# sil_list = list()\n",
    "print(\"range 2-10, step 2\") #4run\n",
    "for k in range(2, 10, 2):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "#     sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "#     sil_list.append(sil)\n",
    "\n",
    "with open(path+\"sse\"+file_name_out+'.pickle', 'wb') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "#     pickle.dump(sil_list, fp)\n",
    "    \n",
    "print(\"range 10-50, step 5\") #8run\n",
    "for k in range(10, 50, 5):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "#     sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "#     sil_list.append(sil)\n",
    "\n",
    "with open(path+\"sse\"+file_name_out+'.pickle', 'ab') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "#     pickle.dump(sil_list, fp)\n",
    "\n",
    "print(\"range 50-200, step 15\") #10run\n",
    "for k in range(50, 200, 15):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "#     sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "#     sil_list.append(sil)\n",
    "\n",
    "with open(path+\"sse\"+file_name_out+'.pickle', 'ab') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "#     pickle.dump(sil_list, fp)\n",
    "    \n",
    "print(\"range 200-1000, step 100\") #4run\n",
    "for k in range(200, 1000, 100):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans.fit(df_corr)\n",
    "    sse = kmeans.inertia_\n",
    "    sse_list.append(sse)\n",
    "#     sil = silhouette_score(df_corr, kmeans.labels_)\n",
    "#     sil_list.append(sil)\n",
    "    \n",
    "with open(path+\"sse\"+file_name_out+'.pickle', 'ab') as fp:\n",
    "    pickle.dump(sse_list, fp)\n",
    "#     pickle.dump(sil_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the pickle files containing the sse and silhouette values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"sse\"+file_name_out+'.pickle', 'rb') as fp:\n",
    "    sse_list1 = pickle.load(fp)\n",
    "#     sil_list1 = pickle.load(fp)\n",
    "    \n",
    "    sse_list2 = pickle.load(fp)\n",
    "#     sil_list2 = pickle.load(fp)\n",
    "\n",
    "    sse_list3 = pickle.load(fp)\n",
    "#     sil_list2 = pickle.load(fp)\n",
    "\n",
    "    sse_list = pickle.load(fp)\n",
    "#     sil_list = pickle.load(fp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the sse and the silhouette values obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw sse\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(20,12)\n",
    "plt.rcParams[\"font.size\"] = '16'\n",
    "x = list(range(2, 10, 2)) + list(range(10, 50, 5)) + list(range(50, 200, 15)) + list(range(200, 1000, 100))\n",
    "\n",
    "plt.plot(x, sse_list, color = '#A8201A', linewidth=2.5)\n",
    "plt.plot(140, sse_list[18], \"o\", color = '#A8201A', markersize = 10) ## area 11\n",
    "# plt.plot(155, sse_list[19], \"o\", color = '#A8201A', markersize = 10) ## area 2\n",
    "\n",
    "plt.xticks(np.arange(0, 1000, 50))\n",
    "plt.yticks(np.arange(15000, 75000, 5000)) ## area 11\n",
    "# plt.yticks(np.arange(50000, 210000, 10000)) ## area 2\n",
    "plt.xlabel(\"k\", fontsize=19)\n",
    "plt.ylabel(\"sse\", fontsize=19)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('../../../thesis/images/sse'+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # draw silhouette\n",
    "# fig = plt.figure()\n",
    "# fig.set_size_inches(20,12)\n",
    "# plt.rcParams[\"font.size\"] = '16'\n",
    "# x = list(range(2,10)) + list(range(10, 200, 5)) + list(range(200, 1000, 100))\n",
    "\n",
    "# plt.plot(x, sil_list, color = '#143642', linewidth=2.5)\n",
    "# # plt.plot(140, sil_list[34], \"o\", color = '#143642', markersize = 10) ## area 11\n",
    "# plt.plot(155, sil_list[37], \"o\", color = '#143642', markersize = 10) ## area 2\n",
    "\n",
    "# # plt.xticks(np.arange(0, 1000, 50)) ## area 11\n",
    "# # plt.yticks(np.arange(0.06, 0.24, 0.01)) ## area 11\n",
    "# plt.xticks(np.arange(0, 1000, 50)) ## area 2\n",
    "# plt.yticks(np.arange(0.06, 0.22, 0.01)) ## area 2\n",
    "\n",
    "# plt.xlabel(\"k\", fontsize=19)\n",
    "# plt.ylabel(\"silhouette\", fontsize=19)\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.savefig('../../../thesis/images/sil'+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the best k for the kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best = 140 ## area 11\n",
    "# k_best = 155 ## area 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run again the kmeans with the k chosen to compute the centroids and the dict from cluster to number of locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ DO NOT RUN AGAIN, TOO LONG ############################\n",
    "kmeans = KMeans(init='k-means++', n_clusters=k_best, n_init=10, max_iter=300, random_state = 123)\n",
    "kmeans.fit(df_corr)\n",
    "\n",
    "# get the centroids\n",
    "centroids_kmeans = kmeans.cluster_centers_\n",
    "labels_kmeans = kmeans.labels_\n",
    "\n",
    "with open(path+\"centroids_kmeans\"+file_name_out+'.pickle', 'wb') as fp:\n",
    "    pickle.dump(centroids_kmeans, fp)\n",
    "    pickle.dump(labels_kmeans, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"centroids_kmeans\"+file_name_out+'.pickle', 'rb') as fp:\n",
    "    centroids_kmeans = pickle.load(fp)\n",
    "    labels_kmeans = pickle.load(fp)\n",
    "    \n",
    "hist, bins = np.histogram(labels_kmeans, bins=range(0, len(set(labels_kmeans)) + 1))\n",
    "# dict from cluster id to number of locs in cluster\n",
    "kmeans_cluster_size = dict(zip(bins, hist)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPUTE THE HIERARCHICAL CLUSTERING ON THE CENTROIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linkage_matrix(centroids_kmeans):\n",
    "    # compute the distance and the linkage matrix\n",
    "    cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "    hierarchy.set_link_color_palette([mpl.colors.rgb2hex(rgb[:3]) for rgb in cmap])\n",
    "\n",
    "    # distance matrix\n",
    "    dist_matrix = pdist(centroids_kmeans, metric='euclidean')\n",
    "    # linkage matrix\n",
    "    link_matrix = linkage(dist_matrix, method='ward', metric='euclidean')\n",
    "    return link_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dict from cluster labels to the points in that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_labels_to_clusters(points, labels):\n",
    "    clusters = defaultdict(list)\n",
    "    for i in range(0, len(points)):\n",
    "        clusters[labels[i]].append(points[i])\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute a set of dictionaries useful for computing measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_dict(link_matrix, centroids_kmeans, labels_kmeans, kmeans_cluster_size):\n",
    "    # list of linkage cluster id \n",
    "    linkage_labels = fcluster(link_matrix, cut_dist, 'distance') \n",
    "    # dict from cluster label to the points in it\n",
    "    linkage_clusters = points_labels_to_clusters(np.array(centroids_kmeans), linkage_labels)\n",
    "\n",
    "    # dict from linkage cluster id to number of locations\n",
    "    link_cluster_to_n_location = dict.fromkeys(np.unique(linkage_labels), 0)\n",
    "    # dict from linkage cluster id to number of kmeans clusters\n",
    "    link_cluster_to_n_kcluster = dict.fromkeys(np.unique(linkage_labels), 0)\n",
    "    # dict from kmeans cluster id to linkage cluster id\n",
    "    kcluster_to_link_cluster = dict.fromkeys(np.unique(labels_kmeans), 0)\n",
    "    for i, c in enumerate(linkage_labels):\n",
    "        link_cluster_to_n_location[c] += kmeans_cluster_size[i]\n",
    "        link_cluster_to_n_kcluster[c] += 1\n",
    "        kcluster_to_link_cluster[i] = c\n",
    "        \n",
    "    return linkage_labels, link_cluster_to_n_location, link_cluster_to_n_kcluster, kcluster_to_link_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to draw the dendrogram of the hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dendro(k, link_matrix, cut_dist, link_cluster_to_n_location):\n",
    "    # draw the dendrogram of the linkage clustering\n",
    "    fig = plt.figure(figsize=(20, 10)) \n",
    "\n",
    "    res = dendrogram(link_matrix, color_threshold = cut_dist, above_threshold_color = 'grey', no_labels= True)\n",
    "    plt.axhline(y=cut_dist, c='r')\n",
    "    y_ticks_max = round(link_matrix[-1][2])\n",
    "    plt.yticks(np.arange(0, y_ticks_max, y_ticks_max/10), fontsize=16)\n",
    "\n",
    "    cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "\n",
    "    legend_handles = []\n",
    "    for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "        legend_handles.append(mpatches.Patch(color=cmap[(i-1)%7], label='C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i])))\n",
    "\n",
    "    plt.legend(handles=legend_handles, loc=1)\n",
    "\n",
    "    plt.savefig('../../../thesis/images/dentro_'+id_area+'_cluster_'+str(k+2)+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe containing the kmeans centroids and linkage clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_centroids(centroids_kmeans, kcluster_to_link_cluster):\n",
    "    # create a dataframe containing the kmeans centroids\n",
    "    df_centroids = pd.DataFrame(centroids_kmeans, columns=df_corr.columns)\n",
    "    # add a column containing for each centroids the linkage cluster id\n",
    "    df_centroids[\"link_cluster\"] = kcluster_to_link_cluster.values()\n",
    "\n",
    "    # for each linkage cluster extract all the centroids and compute a mean\n",
    "    link_centroids = []\n",
    "    link_centroids_std = []\n",
    "    for i in range(1, len(link_cluster_to_n_kcluster)+1):\n",
    "        df_i = df_centroids[df_centroids[\"link_cluster\"] == i]\n",
    "        link_centroids.append(list(df_i.mean(axis = 0)))\n",
    "        link_centroids_std.append(list(df_i.std(axis = 0)))\n",
    "        \n",
    "    # create a dataframe containing of each linkage cluster the mean of the centroids in it\n",
    "    df_par = pd.DataFrame(link_centroids, columns=df_centroids.columns)\n",
    "    \n",
    "    return df_centroids, df_par, link_centroids_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ff5e00', '#cdff00', '#00ff00', '#00ffcc', '#005eff', '#7000ff', '#ff00bf']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "[mpl.colors.rgb2hex(rgb[:3]) for rgb in cmap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the parallel coordinates of the cluster obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# USING PANDAS LIBRARY\n",
    "def draw_par_coords(df_par, link_cluster_to_n_location, cluster_id):\n",
    "    # draw the parallel coordinates of the linkage clusters\n",
    "    fig = plt.figure(figsize=(35, 12)) \n",
    "\n",
    "    cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "\n",
    "    parallel_coordinates(df_par, 'link_cluster', color = cmap, linewidth=3, axvlines=True, \\\n",
    "                         axvlines_kwds={\"linewidth\":0.5, \"color\":\"k\"} )\n",
    "    plt.xticks(rotation=90, fontsize=16)\n",
    "    plt.yticks(np.arange(0, 1.01, 0.1), fontsize=16)\n",
    "\n",
    "    legend_handles = []\n",
    "    for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "        legend_handles.append(mpatches.Patch(color=cmap[(i-1)%7], label='C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i])))\n",
    "\n",
    "    plt.legend(handles=legend_handles, loc=1)\n",
    "    \n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.savefig('../../../thesis/images/parallel_coord_'+id_area+'_cluster_'+str(cluster_id+2)+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### USING ERRORBAR\n",
    "def draw_par_coords_error(df_par, link_centroids_std, link_cluster_to_n_location, cluster_id):\n",
    "    \n",
    "    # draw the parallel coordinates of the linkage clusters\n",
    "    fig, ax = plt.subplots(1,1, figsize=(35, 12)) \n",
    "    #fig = plt.figure() \n",
    "    cmap = cm.gist_rainbow(np.linspace(0.1, 1, 7))\n",
    "    l = len(df_par.keys()[:-1])\n",
    "        \n",
    "    for i, row in df_par.iterrows():\n",
    "        \n",
    "        x = [x + y for x, y in zip(range(l), np.ones(l)*0.05*i)] #df_par.keys()[:-1]\n",
    "        y = row[:-1]\n",
    "        yerr = link_centroids_std[i][:-1]\n",
    "        \n",
    "        (_, caps, _) = plt.errorbar(x, y, yerr=yerr, color=cmap[i], linewidth=5, barsabove=True, \\\n",
    "                            elinewidth=2, uplims=True, lolims=True, label='uplims=True, lolims=True')\n",
    "\n",
    "        for cap in caps:\n",
    "            cap.set_marker(\"_\")\n",
    "            cap.set_markersize(10)\n",
    "            cap.set_markeredgewidth(3)\n",
    "        \n",
    "    x_ticks_labels = df_par.keys()[:-1]\n",
    "    ax.set_xticks(x) # Set number of ticks for x-axis\n",
    "    ax.set_xticklabels(x_ticks_labels, rotation='vertical', fontsize=16) # Set ticks labels for x-axis\n",
    "    plt.yticks(np.arange(-0.2, 1.3, 0.1), fontsize=16)\n",
    "\n",
    "    legend_handles = []\n",
    "    for i in range(1, len(link_cluster_to_n_location)+1):\n",
    "        legend_handles.append('C'+str(i)+', n_locs ='+str(link_cluster_to_n_location[i]))\n",
    "\n",
    "    plt.legend(legend_handles, loc=1)\n",
    "\n",
    "    plt.savefig('../../../thesis/images/parallel_coord_'+id_area+'_cluster_'+str(cluster_id+2)+'_error.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the clusters splitting recursively according to the dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_matrix = compute_linkage_matrix(centroids_kmeans)\n",
    "\n",
    "for i in range(5):\n",
    "    cut_dist = link_matrix[-i-1][2] - 0.1\n",
    "    \n",
    "    linkage_labels, link_cluster_to_n_location, link_cluster_to_n_kcluster, kcluster_to_link_cluster = clusters_dict(\n",
    "                        link_matrix, centroids_kmeans, labels_kmeans, kmeans_cluster_size)\n",
    "    \n",
    "    draw_dendro(i, link_matrix, cut_dist, link_cluster_to_n_location)    \n",
    "    \n",
    "    df_centroids, df_par, link_centroids_std = create_df_centroids(centroids_kmeans, kcluster_to_link_cluster)\n",
    "    \n",
    "    draw_par_coords_error(df_par, link_centroids_std, link_cluster_to_n_location, i)\n",
    "    draw_par_coords(df_par, link_cluster_to_n_location, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAW THE HEATMAP OF THE LOCATIONS COMPOSING THE CLUSTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use selenium to transform a hmtl map into a png image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.PhantomJS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataframe not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe not normalized\n",
    "path = '../../../datasets/out/Traj' + stop + 'min/'\n",
    "file_name_in = 'loc_feat_area'+id_area+'_month'+month_code+'_week'+ week + '_complete.csv'\n",
    "\n",
    "df = pd.read_csv(path+file_name_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_denorm = df[[\"loc_proto_lat\", \"loc_proto_lon\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the array of linkage cluster label for each location and assign the linkage cluster to the dataset of the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_cluster = []\n",
    "for kmeans_label in labels_kmeans:\n",
    "    link_cluster.append(kcluster_to_link_cluster[kmeans_label])\n",
    "    \n",
    "df_locs = df_denorm.copy()\n",
    "df_locs[\"link_cluster\"] = link_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the linkage cluster label for each location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"link_cluster\"+file_name_out+'.pickle', 'wb') as fp:\n",
    "    df_link = df.copy()\n",
    "    df_link = df_link[['vehicle', 'loc_id']]\n",
    "    df_link[\"link_cluster\"] = link_cluster\n",
    "    pickle.dump(df_link, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"link_cluster\"+file_name_out+'.pickle', 'rb') as fp:\n",
    "    df_link = pickle.load(fp)\n",
    "    link_cluster = df_link[\"link_cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all the points of the locations of a linkage cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array with k (linkage) elements, each contains a list of points in that cluster\n",
    "link_points = []\n",
    "for i in range(1, len(np.unique(link_cluster))+1):\n",
    "    df_i = df_locs[df_locs[\"link_cluster\"] == i]\n",
    "    link_points.append([list(a) for a in zip(df_i[\"loc_proto_lat\"], df_i[\"loc_proto_lon\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the heatmap for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(link_points)):\n",
    "    cluster_id = i\n",
    "    m = folium.Map(location=[38, 23], zoom_start=10) ## area 2\n",
    "    #m = folium.Map(location=[38, 23.68], zoom_start=12) ## area 11\n",
    "\n",
    "    # Plot it on the map\n",
    "    HeatMap(link_points[cluster_id]).add_to(m)\n",
    "\n",
    "    folium.map.Marker([38.8, 22.9], ## area 11 [38.2, 23.68] ## area 2 [38.8, 22.9]\n",
    "        icon=folium.features.DivIcon(icon_size=(500,40), icon_anchor=(0,0),\n",
    "                                     html='<div style=\"font-size: 56pt\">CLUSTER '+str(cluster_id+1)+'</div>')).add_to(m)\n",
    "    # Display the map\n",
    "    m.save('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.html')\n",
    "        \n",
    "    driver.set_window_size(2500, 1800)\n",
    "    driver.get('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.html')\n",
    "    driver.save_screenshot('../../../thesis/images/heatmap_area_'+id_area+'_cluster_'+str(cluster_id+1)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PURITY AND ENTROPY DISTRIBUTION OF ANNOTATED IMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df[[\"vehicle\", \"loc_id\", \"support\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a df with the support for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_lin = df_result.copy()\n",
    "df_result_lin[\"link_cluster\"] = link_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute the entropy of the cluster distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(n_locs, tot_locs):\n",
    "    e = 0\n",
    "    for j in range(len(n_locs)):\n",
    "        if n_locs[j] == 0:\n",
    "            continue\n",
    "        p = n_locs[j] / tot_locs\n",
    "        e -= p * np.log2(p)\n",
    "    return e / np.log2(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the list of vehicletypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('../../../datasets/in/Traj'+stop+'min/area'+id_area+'_month'+month_code+'_week'+ week+'_stops.csv')\n",
    "vehicletypes = []\n",
    "v_list = df_temp[\"vehicle\"].unique()\n",
    "for v in v_list:\n",
    "    df_i = df_temp[df_temp[\"vehicle\"] == v]\n",
    "    vehicletypes.append(df_i[\"vehicletype\"].unique()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe containing the count of locations and points in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = []\n",
    "\n",
    "\n",
    "for i, v in enumerate(df_result_lin[\"vehicle\"].unique()):\n",
    "    # get the df of that vehicle only\n",
    "    row = [v]\n",
    "    df_v = df_result_lin[df_result_lin[\"vehicle\"] == v]\n",
    "    \n",
    "    # get the vehicletype\n",
    "    row.append(vehicletypes[i])\n",
    "    \n",
    "    # compute the number of location in each cluster\n",
    "    n_locs = []\n",
    "    for lc in range(1,7):\n",
    "        n_locs.append(len(df_v[df_v[\"link_cluster\"] == lc]))\n",
    "    row.extend(n_locs)\n",
    "    row.append(len(df_v))\n",
    "    # compute the purity of the locations distribution\n",
    "    pur_loc = np.max(n_locs)/len(df_v)\n",
    "    row.append(pur_loc)\n",
    "    # compute the entropy of the locations distribution\n",
    "    row.append(entropy(n_locs, len(df_v)))\n",
    "    \n",
    "    # compute the number of points in each cluster\n",
    "    n_p = []\n",
    "    for lc in range(1,7):\n",
    "        df_lc = df_v[df_v[\"link_cluster\"] == lc]\n",
    "        n_p.append(np.sum(df_lc[\"support\"]))\n",
    "    row.extend(n_p)\n",
    "    row.append(np.sum(n_p))\n",
    "    # compute the purity of the points distribution\n",
    "    pur_loc = np.max(n_p)/np.sum(n_p)\n",
    "    row.append(pur_loc)\n",
    "    # compute the entropy of the locations distribution\n",
    "    row.append(entropy(n_p, np.sum(n_p)))\n",
    "    \n",
    "    df_array.append(row)\n",
    "\n",
    "columns_name = [\"vehicle\", \"vehicletype\", \"n_loc_cluster_1\", \"n_loc_cluster_2\", \"n_loc_cluster_3\", \"n_loc_cluster_4\", \n",
    "                \"n_loc_cluster_5\", \"n_loc_cluster_6\", \"tot_loc\", \"purity_loc\", \"entropy_loc\",\n",
    "                \"n_p_cluster_1\", \"n_p_cluster_2\", \"n_p_cluster_3\", \"n_p_cluster_4\", \"n_p_cluster_5\", \n",
    "                \"n_p_cluster_6\",\"tot_p\", \"purity_p\", \"entropy_p\"]\n",
    "df_purity = pd.DataFrame(df_array, columns=columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purity.to_csv(path+\"df_purity\"+file_name_out+'.csv', mode = \"w\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw the distribution of the purity and the entropy for the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 15\n",
    "\n",
    "xlabels = [\"locations purity\", \"locations entropy\", \"points purity\", \"points entropy\",]\n",
    "pur_entrop = [\"purity_loc\", \"entropy_loc\", \"purity_p\", \"entropy_p\"]\n",
    "colors = ['#fbb4ae', \"#b3cde3\", \"#fed9a6\", \"#ccebc5\"]\n",
    "\n",
    "for i, pe in enumerate(pur_entrop):\n",
    "    fig = plt.figure(figsize=(6, 4)) \n",
    "\n",
    "    y = sorted(df_purity[pe])\n",
    "    _, bins, _ = plt.hist(y, bins=np.arange(0,1.1, 0.1), color = colors[i], ec='#FFFFFF')\n",
    "    plt.xlabel(xlabels[i], fontsize=17)\n",
    "    plt.ylabel(\"number of vehicles\", fontsize=17)\n",
    "\n",
    "    plt.savefig('../../../thesis/images/results/'+pe+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### draw the distribution of the purity and the entropy for the locations (STACKED over the vehicletype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle</th>\n",
       "      <th>vehicletype</th>\n",
       "      <th>n_loc_cluster_1</th>\n",
       "      <th>n_loc_cluster_2</th>\n",
       "      <th>n_loc_cluster_3</th>\n",
       "      <th>n_loc_cluster_4</th>\n",
       "      <th>n_loc_cluster_5</th>\n",
       "      <th>n_loc_cluster_6</th>\n",
       "      <th>tot_loc</th>\n",
       "      <th>purity_loc</th>\n",
       "      <th>entropy_loc</th>\n",
       "      <th>n_p_cluster_1</th>\n",
       "      <th>n_p_cluster_2</th>\n",
       "      <th>n_p_cluster_3</th>\n",
       "      <th>n_p_cluster_4</th>\n",
       "      <th>n_p_cluster_5</th>\n",
       "      <th>n_p_cluster_6</th>\n",
       "      <th>tot_p</th>\n",
       "      <th>purity_p</th>\n",
       "      <th>entropy_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9520_144560</td>\n",
       "      <td>Truck 3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.581381</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>133</td>\n",
       "      <td>0.669173</td>\n",
       "      <td>0.561626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7790_110550</td>\n",
       "      <td>Van</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.679405</td>\n",
       "      <td>921</td>\n",
       "      <td>271</td>\n",
       "      <td>264</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>1544</td>\n",
       "      <td>0.596503</td>\n",
       "      <td>0.608010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12780_115050</td>\n",
       "      <td>Van</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>125</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.486030</td>\n",
       "      <td>2099</td>\n",
       "      <td>140</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.902408</td>\n",
       "      <td>0.240101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11580_105930</td>\n",
       "      <td>Truck 3</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.686874</td>\n",
       "      <td>1057</td>\n",
       "      <td>299</td>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.709396</td>\n",
       "      <td>0.471812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>740_11220</td>\n",
       "      <td>Van</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.533386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.473660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>16480_142990</td>\n",
       "      <td>Van</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.676146</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>169</td>\n",
       "      <td>0.639053</td>\n",
       "      <td>0.639694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>16020_139450</td>\n",
       "      <td>Van</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.658741</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.635659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>13030_121960</td>\n",
       "      <td>Truck 3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.601405</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>0.556617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>15990_139370</td>\n",
       "      <td>Van</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>3270_41350</td>\n",
       "      <td>Truck 3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          vehicle vehicletype  n_loc_cluster_1  n_loc_cluster_2  \\\n",
       "0     9520_144560     Truck 3                2                3   \n",
       "1     7790_110550         Van               47                8   \n",
       "2    12780_115050         Van               84                3   \n",
       "3    11580_105930     Truck 3               20                5   \n",
       "4       740_11220         Van                0                0   \n",
       "..            ...         ...              ...              ...   \n",
       "274  16480_142990         Van                4                0   \n",
       "275  16020_139450         Van                3                2   \n",
       "276  13030_121960     Truck 3                0                2   \n",
       "277  15990_139370         Van                0                0   \n",
       "278    3270_41350     Truck 3                0                0   \n",
       "\n",
       "     n_loc_cluster_3  n_loc_cluster_4  n_loc_cluster_5  n_loc_cluster_6  \\\n",
       "0                  1                0               15                2   \n",
       "1                 14                3               25                0   \n",
       "2                  3                3                9                6   \n",
       "3                  4                1               10                0   \n",
       "4                  1                2                4                0   \n",
       "..               ...              ...              ...              ...   \n",
       "274                2                9                9                1   \n",
       "275                0                2                8                0   \n",
       "276                0                1                9                3   \n",
       "277                0                0                2                0   \n",
       "278                0                0                3                0   \n",
       "\n",
       "     tot_loc  purity_loc  entropy_loc  n_p_cluster_1  n_p_cluster_2  \\\n",
       "0         31    0.483871     0.581381             10             89   \n",
       "1        120    0.391667     0.679405            921            271   \n",
       "2        125    0.672000     0.486030           2099            140   \n",
       "3         46    0.434783     0.686874           1057            299   \n",
       "4          7    0.571429     0.533386              0              0   \n",
       "..       ...         ...          ...            ...            ...   \n",
       "274       35    0.257143     0.676146            108              0   \n",
       "275       16    0.500000     0.658741             20             54   \n",
       "276       15    0.600000     0.601405              0             31   \n",
       "277        2    1.000000     0.000000              0              0   \n",
       "278        3    1.000000     0.000000              0              0   \n",
       "\n",
       "     n_p_cluster_3  n_p_cluster_4  n_p_cluster_5  n_p_cluster_6  tot_p  \\\n",
       "0                5              0             25              4    133   \n",
       "1              264              4             84              0   1544   \n",
       "2               40              6             24             17   2326   \n",
       "3               95              4             35              0   1490   \n",
       "4                4              2             12              0     18   \n",
       "..             ...            ...            ...            ...    ...   \n",
       "274             16             18             17             10    169   \n",
       "275              0              4             32              0    110   \n",
       "276              0              2             17              4     54   \n",
       "277              0              0              4              0      4   \n",
       "278              0              0              4              0      4   \n",
       "\n",
       "     purity_p  entropy_p  \n",
       "0    0.669173   0.561626  \n",
       "1    0.596503   0.608010  \n",
       "2    0.902408   0.240101  \n",
       "3    0.709396   0.471812  \n",
       "4    0.666667   0.473660  \n",
       "..        ...        ...  \n",
       "274  0.639053   0.639694  \n",
       "275  0.490909   0.635659  \n",
       "276  0.574074   0.556617  \n",
       "277  1.000000   0.000000  \n",
       "278  1.000000   0.000000  \n",
       "\n",
       "[279 rows x 20 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.rcParams[\"font.size\"] = 15\n",
    "\n",
    "xlabels = [\"locations purity\", \"locations entropy\", \"points purity\", \"points entropy\",]\n",
    "pur_entrop = [\"purity_loc\", \"entropy_loc\", \"purity_p\", \"entropy_p\"]\n",
    "colors = ['#fbb4ae', \"#b3cde3\", \"#fed9a6\", \"#ccebc5\"]\n",
    "\n",
    "for i, pe in enumerate(pur_entrop):\n",
    "    fig = plt.figure(figsize=(6, 4)) \n",
    "\n",
    "    y = sorted(df_purity[pe])\n",
    "    _, bins, _ = plt.hist(y, bins=np.arange(0,1.1, 0.1), color = colors[i], ec='#FFFFFF', stacked=True)\n",
    "    plt.xlabel(xlabels[i], fontsize=17)\n",
    "    plt.ylabel(\"number of vehicles\", fontsize=17)\n",
    "\n",
    "    plt.savefig('../../../thesis/images/results/'+pe+\"_stacked_\"+file_name_out+'.png', format='png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
